{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant packages for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['JAX_PMAP_USE_TENSORSTORE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimesFM v1.2.0. See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Loaded Jax TimesFM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 11:29:48.811050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import timesfm\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timesfm import patched_decoder\n",
    "from timesfm import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TimesFM pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28838d5a34442af9d35e6a748debcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 11:30:02.081064: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.6.85). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing context has already been set.\n",
      "Constructing model weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No registered CheckpointArgs found for handler type: <class 'paxml.checkpoints.FlaxCheckpointHandler'>\n",
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n",
      "WARNING:absl:train_state_unpadded_shape_dtype_struct is not provided. We assume `train_state` is unpadded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed model weights in 1.45 seconds.\n",
      "Restoring checkpoint from /home/ming/.cache/huggingface/hub/models--google--timesfm-1.0-200m/snapshots/8775f7531211ac864b739fe776b0b255c277e2be/checkpoints.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:For checkpoint version > 1.0, we require users to provide\n",
      "          `train_state_unpadded_shape_dtype_struct` during checkpoint\n",
      "          saving/restoring, to avoid potential silent bugs when loading\n",
      "          checkpoints to incompatible unpadded shapes of TrainState.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored checkpoint in 0.63 seconds.\n",
      "Jitting decoding.\n",
      "Jitted decoding in 9.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "tfm = timesfm.TimesFm(\n",
    "      hparams=timesfm.TimesFmHparams(\n",
    "          backend=\"gpu\",\n",
    "          per_core_batch_size=32,\n",
    "          horizon_len=128,\n",
    "      ),\n",
    "      checkpoint=timesfm.TimesFmCheckpoint(\n",
    "          huggingface_repo_id=\"google/timesfm-1.0-200m\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating pretrained checkpoint on ETT datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA_DICT = {\n",
    "    \"ettm2\": {\n",
    "        \"boundaries\": [34560, 46080, 57600],\n",
    "        \"data_path\": \"../datasets/ETT-small/ETTm2.csv\",\n",
    "        \"freq\": \"15min\",\n",
    "    },\n",
    "    \"ettm1\": {\n",
    "        \"boundaries\": [34560, 46080, 57600],\n",
    "        \"data_path\": \"../datasets/ETT-small/ETTm1.csv\",\n",
    "        \"freq\": \"15min\",\n",
    "    },\n",
    "    \"etth2\": {\n",
    "        \"boundaries\": [8640, 11520, 14400],\n",
    "        \"data_path\": \"../datasets/ETT-small/ETTh2.csv\",\n",
    "        \"freq\": \"H\",\n",
    "    },\n",
    "    \"etth1\": {\n",
    "        \"boundaries\": [8640, 11520, 14400],\n",
    "        \"data_path\": \"../datasets/ETT-small/ETTh1.csv\",\n",
    "        \"freq\": \"H\",\n",
    "    },\n",
    "    \"elec\": {\n",
    "        \"boundaries\": [18413, 21044, 26304],\n",
    "        \"data_path\": \"../datasets/electricity/electricity.csv\",\n",
    "        \"freq\": \"H\",\n",
    "    },\n",
    "    \"traffic\": {\n",
    "        \"boundaries\": [12280, 14036, 17544],\n",
    "        \"data_path\": \"../datasets/traffic/traffic.csv\",\n",
    "        \"freq\": \"H\",\n",
    "    },\n",
    "    \"weather\": {\n",
    "        \"boundaries\": [36887, 42157, 52696],\n",
    "        \"data_path\": \"../datasets/weather/weather.csv\",\n",
    "        \"freq\": \"10min\",\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "DATA_DICT = {\n",
    "    \"ettm1\": {\n",
    "        \"boundaries\": [80, 100, 115],#train,val,test\n",
    "        \"data_path\": \"/home/ming/aaguolishaData/time_xulie/timedata.csv\",\n",
    "        \"freq\": \"D\",\n",
    "    }}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "补全数据之前的数据条数: 116\n",
      "        date  value\n",
      "0  2014/7/21      1\n",
      "1   2014/8/8      1\n",
      "2  2014/8/12      2\n",
      "3  2014/8/16      2\n",
      "4  2014/8/18      2\n",
      "补全数据之后的数据条数: 164\n",
      "        date  value\n",
      "0 2014-07-21      1\n",
      "1 2014-07-22      0\n",
      "2 2014-07-23      0\n",
      "3 2014-07-24      0\n",
      "4 2014-07-25      0\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ettm1\"\n",
    "data_path = DATA_DICT[dataset][\"data_path\"]\n",
    "freq = DATA_DICT[dataset][\"freq\"]\n",
    "int_freq = timesfm.freq_map(freq)\n",
    "boundaries = DATA_DICT[dataset][\"boundaries\"]\n",
    "\n",
    "#data_df = pd.read_csv(open(data_path, \"r\"))\n",
    "\n",
    "# 加载数据并处理缺失日期\n",
    "data_df = pd.read_csv(open(data_path, \"r\"))\n",
    "\n",
    "# 输出补全数据之前的数据条数\n",
    "print(\"补全数据之前的数据条数:\", len(data_df))\n",
    "print(data_df.head())\n",
    "\n",
    "data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "date_range = pd.date_range(start=data_df['date'].min(), end=data_df['date'].max(), freq='D')\n",
    "data_df = data_df.set_index('date').reindex(date_range, fill_value=0).rename_axis('date').reset_index()\n",
    "\n",
    "# 输出补全数据之后的数据条数\n",
    "print(\"补全数据之后的数据条数:\", len(data_df))\n",
    "# 查看处理后的数据\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ts_cols = [col for col in data_df.columns if col != \"date\"]\n",
    "ts_cols = ['value']\n",
    "num_cov_cols = []\n",
    "cat_cov_cols = []\n",
    "\n",
    "#num_cov_cols = None\n",
    "#cat_cov_cols = None\n",
    "\n",
    "\n",
    "context_len = 30\n",
    "pred_len = 5\n",
    "\n",
    "num_ts = len(ts_cols)\n",
    "print(\"^^^^^^\")\n",
    "print(num_ts)\n",
    "batch_size = 2\n",
    "\n",
    "#初始化一个时间序列数据加载器对象 dtl，用于加载和处理时间序列数据\n",
    "dtl = data_loader.TimeSeriesdata(\n",
    "      data_path=data_path,\n",
    "      datetime_col=\"date\",\n",
    "      num_cov_cols=num_cov_cols,\n",
    "      cat_cov_cols=cat_cov_cols,\n",
    "      ts_cols=np.array(ts_cols),\n",
    "\n",
    "      train_range=[0, boundaries[0]],\n",
    "      val_range=[boundaries[0], boundaries[1]],\n",
    "      test_range=[boundaries[1], boundaries[2]],\n",
    "      \n",
    "      hist_len=context_len,\n",
    "      pred_len=pred_len,\n",
    "      batch_size=num_ts,\n",
    "      freq=freq,\n",
    "      normalize=False,\n",
    "      epoch_len=None,\n",
    "      holiday=False,\n",
    "      permute=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.columns)#确认有没有协变量(查看是否存在除了 date 和 ts_cols 之外的其他列)\n",
    "\n",
    "##查看原始训练数据的前10行\n",
    "train_range = [0, boundaries[0]]\n",
    "# 假设 data_df 是加载原始数据框\n",
    "# 使用 train_range 提取训练数据\n",
    "train_data = data_df.iloc[train_range[0]:train_range[1]]\n",
    "\n",
    "print(train_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间序列数据加载器对象 dtl\n",
    "##dtl.tf_dataset() 方法将原始数据转换为 TensorFlow 数据集（tf.data.Dataset 对象）\n",
    "train_batches = dtl.tf_dataset(mode=\"train\", shift=1).batch(batch_size)\n",
    "val_batches = dtl.tf_dataset(mode=\"val\", shift=pred_len)\n",
    "test_batches = dtl.tf_dataset(mode=\"test\", shift=pred_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何理解batch?它的形状是什么？\n",
    " (batch_size, num_ts, context_len)，其中：\n",
    "\n",
    "batch_size 是批次大小。\n",
    "num_ts 是时间序列的数量。\n",
    "context_len 是历史数据长度\n",
    "\n",
    "batch 是一个包含多个元素的元组或列表，每个元素代表不同的部分。例如：\n",
    "\n",
    "batch[0]：历史数据（past），用于模型输入。\n",
    "batch[1]：协变量数据（covariates），通常是额外的特征数据。\n",
    "batch[2]：目标值（targets），用于模型训练或评估。\n",
    "batch[3]：实际值（actuals），用于与模型预测值进行比较。\n",
    "因此，batch[3] 是 batch 的第四个元素，表示实际值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbatch in tqdm(train_batches.as_numpy_iterator()):\n",
    "    pass\n",
    "print(tbatch[0].shape)\n",
    "\n",
    "#tiao_shi_dai_ma\n",
    "# 遍历训练集数据批次并打印最后一个批次中第一个元素的形状\n",
    "for tbatch in tqdm(train_batches.as_numpy_iterator()):\n",
    "    #train_batches.as_numpy_iterator() 将 TensorFlow 数据集转换为 NumPy 数组\n",
    "    # 打印第一个批次的数据\n",
    "    print(\"tbatch[0]\\n\", tbatch[0])#\"历史数据（past）\n",
    "    print(\"tbatch[1]\\n\", tbatch[1])#变量数据（covariates）\n",
    "    print(\"tbatch[2]\\n\", tbatch[2])#目标值（targets\n",
    "    break  # 只打印第一个批次，避免输出过多数据\n",
    "print(\"最后一个批次中第一个元素的形状：\", tbatch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE on the test split for the pretrained TimesFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_losses = []\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "\n",
    "    forecasts, _ = tfm.forecast(list(past), [0] * past.shape[0], normalize=True)\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1]]\n",
    "    mae_losses.append(np.abs(forecasts - actuals).mean())\n",
    "\n",
    "print(f\"MAE: {np.mean(mae_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#我改代码\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "\n",
    "    # 输出查看 past 和 actuals\n",
    "    print(\"历史数据（past）：\", past)\n",
    "    print(\"实际值（actuals）：\", actuals)\n",
    "    \n",
    "    forecasts, _ = tfm.forecast(list(past), [0] * past.shape[0], normalize=True)\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1]]\n",
    "    \n",
    "    # 输出预测值\n",
    "    print(\"预测值（forecasts）：\", forecasts)\n",
    "\n",
    "    mae_losses.append(np.abs(forecasts - actuals).mean())\n",
    "    break  # 如果只想查看第一个批次, 可以使用 break 退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_losses = []\n",
    "sample_count = 0  # 用于记录样本编号\n",
    "\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n",
    "    \n",
    "    # 计算当前样本的 MAE\n",
    "    sample_mae = np.abs(forecasts - actuals).mean()\n",
    "    mae_losses.append(sample_mae)\n",
    "    \n",
    "    # 输出当前样本的信息\n",
    "    sample_count += 1\n",
    "    print(f\"样本{sample_count}：\")\n",
    "    print(\"历史数据（past）：\", past.tolist())  # 将数组转换为列表以便输出\n",
    "    print(\"实际值（actuals）：\", actuals.tolist())\n",
    "    print(\"预测值（forecasts）：\", forecasts.tolist())\n",
    "    print(f\"样本{sample_count} MAE: {sample_mae}\")\n",
    "    print(\"-\" * 50)  # 分隔线\n",
    "\n",
    "# 输出所有样本的平均 MAE\n",
    "mae_mean = np.mean(mae_losses)\n",
    "print(f\"所有样本平均 MAE: MAE_mean: {mae_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the model on the ETT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from praxis import pax_fiddle\n",
    "from praxis import py_utils\n",
    "from praxis import pytypes\n",
    "from praxis import base_model\n",
    "from praxis import optimizers\n",
    "from praxis import schedules\n",
    "from praxis import base_hyperparams\n",
    "from praxis import base_layer\n",
    "from paxml import tasks_lib\n",
    "from paxml import trainer_lib\n",
    "from paxml import checkpoints\n",
    "from paxml import learners\n",
    "from paxml import partitioning\n",
    "from paxml import checkpoint_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAX shortcuts\n",
    "NestedMap = py_utils.NestedMap\n",
    "WeightInit = base_layer.WeightInit\n",
    "WeightHParams = base_layer.WeightHParams\n",
    "InstantiableParams = py_utils.InstantiableParams\n",
    "JTensor = pytypes.JTensor\n",
    "NpTensor = pytypes.NpTensor\n",
    "WeightedScalars = pytypes.WeightedScalars\n",
    "instantiate = base_hyperparams.instantiate\n",
    "LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]\n",
    "AuxLossStruct = base_layer.AuxLossStruct\n",
    "\n",
    "AUX_LOSS = base_layer.AUX_LOSS\n",
    "template_field = base_layer.template_field\n",
    "\n",
    "# Standard prng key names\n",
    "PARAMS = base_layer.PARAMS\n",
    "RANDOM = base_layer.RANDOM\n",
    "\n",
    "key = jax.random.PRNGKey(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pax_fiddle.Config(\n",
    "    patched_decoder.PatchedDecoderFinetuneModel,\n",
    "    name='patched_decoder_finetune',\n",
    "    core_layer_tpl=tfm.model_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will hold the transformer layers fixed while finetuning, while training all other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pax_fiddle.auto_config\n",
    "def build_learner() -> learners.Learner:\n",
    "  return pax_fiddle.Config(\n",
    "      learners.Learner,\n",
    "      name='learner',\n",
    "      loss_name='avg_qloss',\n",
    "      optimizer=optimizers.Adam(\n",
    "          epsilon=1e-7,\n",
    "          clip_threshold=1e2,\n",
    "          learning_rate=1e-2,\n",
    "          lr_schedule=pax_fiddle.Config(\n",
    "              schedules.Cosine,\n",
    "              initial_value=1e-3,\n",
    "              final_value=1e-4,\n",
    "              total_steps=40000,\n",
    "          ),\n",
    "          ema_decay=0.9999,\n",
    "      ),\n",
    "      # Linear probing i.e we hold the transformer layers fixed.\n",
    "      bprop_variable_exclusion=['.*/stacked_transformer_layer/.*'],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_p = tasks_lib.SingleTask(\n",
    "    name='ts-learn',\n",
    "    model=model,\n",
    "    train=tasks_lib.SingleTask.Train(\n",
    "        learner=build_learner(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_p.model.ici_mesh_shape = [1, 1, 1]\n",
    "task_p.model.mesh_axis_names = ['replica', 'data', 'mdl']\n",
    "\n",
    "DEVICES = np.array(jax.devices()).reshape([1, 1, 1])\n",
    "MESH = jax.sharding.Mesh(DEVICES, ['replica', 'data', 'mdl'])\n",
    "\n",
    "num_devices = jax.local_device_count()\n",
    "print(f'num_devices: {num_devices}')\n",
    "print(f'device kind: {jax.local_devices()[0].device_kind}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[0].shape)\n",
    "\n",
    "jax_task = task_p\n",
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "# To correctly prepare a batch of data for model initialization (now that shape\n",
    "# inference is merged), we take one devices*batch_size tensor tuple of data,\n",
    "# slice out just one batch, then run the prepare_input_batch function over it.\n",
    "\n",
    "\n",
    "def process_train_batch(batch):\n",
    "    past_ts = batch[0].reshape(batch_size * num_ts, -1)\n",
    "    actual_ts = batch[3].reshape(batch_size * num_ts, -1)\n",
    "    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n",
    "\n",
    "\n",
    "def process_eval_batch(batch):\n",
    "    past_ts = batch[0]\n",
    "    actual_ts = batch[3]\n",
    "    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n",
    "\n",
    "\n",
    "jax_model_states, _ = trainer_lib.initialize_model_state(\n",
    "    jax_task,\n",
    "    init_key,\n",
    "    process_train_batch(tbatch),\n",
    "    checkpoint_type=checkpoint_types.CheckpointType.GDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the initial model weights to the pretrained TimesFM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_model_states.mdl_vars['params']['core_layer'] = tfm._train_state.mdl_vars['params']\n",
    "jax_vars = jax_model_states.mdl_vars\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_task = task_p\n",
    "\n",
    "\n",
    "def train_step(states, prng_key, inputs):\n",
    "  return trainer_lib.train_step_single_learner(\n",
    "      jax_task, states, prng_key, inputs\n",
    "  )\n",
    "\n",
    "\n",
    "def eval_step(states, prng_key, inputs):\n",
    "  states = states.to_eval_state()\n",
    "  return trainer_lib.eval_step_single_learner(\n",
    "      jax_task, states, prng_key, inputs\n",
    "  )\n",
    "\n",
    "key, train_key, eval_key = jax.random.split(key, 3)\n",
    "train_prng_seed = jax.random.split(train_key, num=jax.local_device_count())\n",
    "eval_prng_seed = jax.random.split(eval_key, num=jax.local_device_count())\n",
    "\n",
    "p_train_step = jax.pmap(train_step, axis_name='batch')\n",
    "p_eval_step = jax.pmap(eval_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicated_jax_states = trainer_lib.replicate_model_state(jax_model_states)\n",
    "replicated_jax_vars = replicated_jax_states.mdl_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = 1e7\n",
    "step_count = 0\n",
    "patience = 0\n",
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "TRAIN_STEPS_PER_EVAL = 1000\n",
    "#CHECKPOINT_DIR='/home/senrajat_google_com/ettm1_finetune'\n",
    "CHECKPOINT_DIR='/home/ming/aaguolishaData/time_xulie/tfs_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_batch_for_pmap(batch, num_devices):\n",
    "  def _reshape(input_tensor):\n",
    "    bsize = input_tensor.shape[0]\n",
    "    residual_shape = list(input_tensor.shape[1:])\n",
    "    nbsize = bsize // num_devices\n",
    "    return jnp.reshape(input_tensor, [num_devices, nbsize] + residual_shape)\n",
    "\n",
    "  return jax.tree.map(_reshape, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil  # 导入 shutil 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"__________________Epoch: {epoch}__________________\", flush=True)\n",
    "    train_its = train_batches.as_numpy_iterator()\n",
    "    if patience >= PATIENCE:\n",
    "        print(\"Early stopping.\", flush=True)\n",
    "        break\n",
    "    for batch in tqdm(train_its):\n",
    "        train_losses = []\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\", flush=True)\n",
    "            break\n",
    "        tbatch = process_train_batch(batch)\n",
    "        # process_train_batch协变量数据的处理\n",
    "        tbatch = reshape_batch_for_pmap(tbatch, num_devices)\n",
    "        replicated_jax_states, step_fun_out = p_train_step(\n",
    "            replicated_jax_states, train_prng_seed, tbatch\n",
    "        )\n",
    "        train_losses.append(step_fun_out.loss[0])\n",
    "        if step_count % TRAIN_STEPS_PER_EVAL == 0:\n",
    "            print(\n",
    "                f\"Train loss at step {step_count}: {np.mean(train_losses)}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            train_losses = []\n",
    "            print(\"Starting eval.\", flush=True)\n",
    "            val_its = val_batches.as_numpy_iterator()\n",
    "            eval_losses = []\n",
    "            for ev_batch in tqdm(val_its):\n",
    "                ebatch = process_eval_batch(ev_batch)\n",
    "                ebatch = reshape_batch_for_pmap(ebatch, num_devices)\n",
    "                _, step_fun_out = p_eval_step(\n",
    "                    replicated_jax_states, eval_prng_seed, ebatch\n",
    "                )\n",
    "                eval_losses.append(step_fun_out.loss[0])\n",
    "            mean_loss = np.mean(eval_losses)\n",
    "            print(f\"Eval loss at step {step_count}: {mean_loss}\", flush=True)\n",
    "            if mean_loss < best_eval_loss or np.isnan(mean_loss):\n",
    "                best_eval_loss = mean_loss\n",
    "                print(\"Saving checkpoint.\")\n",
    "                jax_state_for_saving = py_utils.maybe_unreplicate_for_fully_replicated(\n",
    "                    replicated_jax_states\n",
    "                )\n",
    "                # 在保存检查点之前，手动删除旧的检查点\n",
    "                checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_1\")\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    shutil.rmtree(checkpoint_path)\n",
    "                ## 保存新的检查点\n",
    "                checkpoints.save_checkpoint(\n",
    "                    jax_state_for_saving, CHECKPOINT_DIR, overwrite=True\n",
    "                )\n",
    "                patience = 0\n",
    "                del jax_state_for_saving\n",
    "                gc.collect()\n",
    "            else:\n",
    "                patience += 1\n",
    "                print(f\"patience: {patience}\")\n",
    "        step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and evaluating the best (according to validation loss) finetuned checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = checkpoints.restore_checkpoint(jax_model_states, CHECKPOINT_DIR)\n",
    "print(train_state.step)\n",
    "tfm._train_state.mdl_vars['params'] = train_state.mdl_vars['params']['core_layer']\n",
    "tfm.jit_decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_losses = []\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n",
    "    mae_losses.append(np.abs(forecasts - actuals).mean())\n",
    "\n",
    "print(f\"MAE: {np.mean(mae_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_losses = []\n",
    "sample_count = 0  # 用于记录样本编号\n",
    "\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n",
    "    \n",
    "    # 计算当前样本的 MAE\n",
    "    sample_mae = np.abs(forecasts - actuals).mean()\n",
    "    mae_losses.append(sample_mae)\n",
    "    \n",
    "    # 输出当前样本的信息\n",
    "    sample_count += 1\n",
    "    print(f\"样本{sample_count}：\")\n",
    "    print(\"历史数据（past）：\", past.tolist())  # 将数组转换为列表以便输出\n",
    "    print(\"实际值（actuals）：\", actuals.tolist())\n",
    "    print(\"预测值（forecasts）：\", forecasts.tolist())\n",
    "    print(f\"样本{sample_count} MAE: {sample_mae}\")\n",
    "    print(\"-\" * 50)  # 分隔线\n",
    "\n",
    "# 输出所有样本的平均 MAE\n",
    "mae_mean = np.mean(mae_losses)\n",
    "print(f\"所有样本平均 MAE: MAE_mean: {mae_mean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is around a __9%__ reduction in MAE from finetuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesfm",
   "language": "python",
   "name": "timesfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
